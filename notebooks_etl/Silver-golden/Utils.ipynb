{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Cargar variables del archivo .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7decfd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conectar_minio(endpoint='localhost:9000', \n",
    "                  access_key=None, \n",
    "                  secret_key=None, \n",
    "                  secure=False):\n",
    "    \"\"\"\n",
    "    Establece una conexión con un servidor MinIO.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    endpoint : str, opcional\n",
    "        Dirección del servidor MinIO (por defecto 'localhost:9000')\n",
    "    access_key : str, requerido\n",
    "        Usuario/access key para la autenticación\n",
    "    secret_key : str, requerido\n",
    "        Contraseña/secret key para la autenticación\n",
    "    secure : bool, opcional\n",
    "        Si es True, usa HTTPS. Si es False, usa HTTP (por defecto False)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    Minio\n",
    "        Cliente MinIO conectado\n",
    "    \n",
    "    Excepciones:\n",
    "    ------------\n",
    "    ValueError\n",
    "        Si no se proporcionan access_key o secret_key\n",
    "    S3Error\n",
    "        Si hay un error al conectar con el servidor MinIO\n",
    "    \"\"\"\n",
    "    \n",
    "    if not access_key or not secret_key:\n",
    "        raise ValueError(\"Se requieren access_key y secret_key para la conexión\")\n",
    "    \n",
    "    try:\n",
    "        # Crear cliente MinIO\n",
    "        cliente = Minio(\n",
    "            endpoint,\n",
    "            access_key=access_key,\n",
    "            secret_key=secret_key,\n",
    "            secure=secure\n",
    "        )\n",
    "        \n",
    "        # Verificar conexión listando los buckets (opcional)\n",
    "        buckets = cliente.list_buckets()\n",
    "        print(f\"Conexión exitosa a MinIO en {endpoint}\")\n",
    "        print(f\"Buckets disponibles: {[bucket.name for bucket in buckets]}\")\n",
    "        \n",
    "        return cliente\n",
    "    \n",
    "    except S3Error as err:\n",
    "        print(f\"Error al conectar con MinIO: {err}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988629b",
   "metadata": {},
   "source": [
    "Subir archivos minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9fa7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO, StringIO\n",
    "import pandas as pd\n",
    "from minio.error import S3Error  # Asegúrate de importar S3Error\n",
    "\n",
    "def guardar_df_en_minio(minio_client, df, bucket_name, ruta_destino, \n",
    "                        formato='parquet', crear_bucket=False):\n",
    "    \"\"\"\n",
    "    Guarda un DataFrame directamente en un bucket de MinIO.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not minio_client:\n",
    "        raise ValueError(\"Se requiere un cliente MinIO válido\")\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"El parámetro df debe ser un pandas.DataFrame\")\n",
    "    if not bucket_name or not ruta_destino:\n",
    "        raise ValueError(\"bucket_name y ruta_destino son requeridos\")\n",
    "    \n",
    "    formatos_soportados = {\n",
    "        'parquet': {\n",
    "            'mime': 'application/parquet',\n",
    "            'writer': lambda buffer: df.to_parquet(buffer, index=False),\n",
    "            'extension': '.parquet',\n",
    "            'buffer_type': BytesIO\n",
    "        },\n",
    "        'csv': {\n",
    "            'mime': 'text/csv',\n",
    "            'writer': lambda buffer: df.to_csv(buffer, index=False),\n",
    "            'extension': '.csv',\n",
    "            'buffer_type': StringIO\n",
    "        },\n",
    "        'json': {\n",
    "            'mime': 'application/json',\n",
    "            'writer': lambda buffer: df.to_json(buffer, orient='records'),\n",
    "            'extension': '.json',\n",
    "            'buffer_type': BytesIO\n",
    "        },\n",
    "        'excel': {\n",
    "            'mime': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
    "            'writer': lambda buffer: df.to_excel(buffer, index=False),\n",
    "            'extension': '.xlsx',\n",
    "            'buffer_type': BytesIO\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    formato = formato.lower()\n",
    "    if formato not in formatos_soportados:\n",
    "        raise ValueError(f\"Formato '{formato}' no soportado. Use: {list(formatos_soportados.keys())}\")\n",
    "    \n",
    "    if not ruta_destino.lower().endswith(formatos_soportados[formato]['extension']):\n",
    "        ruta_destino += formatos_soportados[formato]['extension']\n",
    "    \n",
    "    try:\n",
    "        if crear_bucket and not minio_client.bucket_exists(bucket_name):\n",
    "            minio_client.make_bucket(bucket_name)\n",
    "            print(f\"Bucket '{bucket_name}' creado exitosamente\")\n",
    "\n",
    "        if not minio_client.bucket_exists(bucket_name):\n",
    "            raise S3Error(f\"El bucket '{bucket_name}' no existe\", bucket_name, None, 404)\n",
    "        \n",
    "        # Crear buffer en memoria\n",
    "        buffer = formatos_soportados[formato]['buffer_type']()\n",
    "        formatos_soportados[formato]['writer'](buffer)\n",
    "\n",
    "        # Ajuste para CSV (convertir a BytesIO)\n",
    "        if formato == 'csv':\n",
    "            buffer.seek(0)\n",
    "            data = BytesIO(buffer.getvalue().encode('utf-8'))\n",
    "            length = len(data.getvalue())\n",
    "        else:\n",
    "            buffer.seek(0)\n",
    "            data = buffer\n",
    "            length = buffer.getbuffer().nbytes\n",
    "        \n",
    "        # Subir a MinIO\n",
    "        minio_client.put_object(\n",
    "            bucket_name=bucket_name,\n",
    "            object_name=ruta_destino,\n",
    "            data=data,\n",
    "            length=length,\n",
    "            content_type=formatos_soportados[formato]['mime']\n",
    "        )\n",
    "        \n",
    "        ruta_completa = f\"{bucket_name}/{ruta_destino}\"\n",
    "        print(f\"DataFrame guardado exitosamente en: {ruta_completa}\")\n",
    "        return ruta_completa\n",
    "\n",
    "    except S3Error as err:\n",
    "        print(f\"Error al guardar DataFrame en MinIO: {err}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el DataFrame: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26d547",
   "metadata": {},
   "source": [
    "Leer archivos Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "def extraer_archivo_minio(minio_client, bucket_name, ruta_archivo, tipo_archivo):\n",
    "    \"\"\"\n",
    "    Extrae un archivo de MinIO y lo retorna en el formato adecuado.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    minio_client : Minio\n",
    "        Cliente MinIO ya conectado\n",
    "    bucket_name : str\n",
    "        Nombre del bucket donde está el archivo\n",
    "    ruta_archivo : str\n",
    "        Ruta completa del archivo dentro del bucket\n",
    "    tipo_archivo : str\n",
    "        Tipo de archivo a extraer ('pdf', 'word', 'excel', 'csv', 'parquet')\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    Depende del tipo de archivo:\n",
    "    - 'pdf': Texto extraído (str)\n",
    "    - 'word': Documento de python-docx\n",
    "    - 'excel': DataFrame de pandas\n",
    "    - 'csv': DataFrame de pandas\n",
    "    - 'parquet': DataFrame de pandas\n",
    "    \n",
    "    Excepciones:\n",
    "    ------------\n",
    "    ValueError\n",
    "        Si los parámetros son inválidos o el tipo no es soportado\n",
    "    S3Error\n",
    "        Si hay un error al acceder al archivo o el bucket no existe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validaciones iniciales\n",
    "    if not minio_client:\n",
    "        raise ValueError(\"Se requiere un cliente MinIO válido\")\n",
    "    if not bucket_name or not ruta_archivo or not tipo_archivo:\n",
    "        raise ValueError(\"bucket_name, ruta_archivo y tipo_archivo son requeridos\")\n",
    "    \n",
    "    tipo_archivo = tipo_archivo.lower()\n",
    "    tipos_soportados = ['pdf', 'word', 'excel', 'csv', 'parquet']\n",
    "    \n",
    "    if tipo_archivo not in tipos_soportados:\n",
    "        raise ValueError(f\"Tipo de archivo '{tipo_archivo}' no soportado. Use: {tipos_soportados}\")\n",
    "    \n",
    "    try:\n",
    "        # Obtener el objeto de MinIO\n",
    "        response = minio_client.get_object(bucket_name, ruta_archivo)\n",
    "        data = BytesIO(response.read())\n",
    "        data.seek(0)\n",
    "        \n",
    "        # Procesar según el tipo de archivo\n",
    "        if tipo_archivo == 'pdf':\n",
    "            # Extraer texto de PDF\n",
    "            pdf_reader = PyPDF2.PdfReader(data)\n",
    "            text = \"\\n\".join([page.extract_text() for page in pdf_reader.pages])\n",
    "            return text\n",
    "        \n",
    "        elif tipo_archivo == 'word':\n",
    "            # Retornar documento de Word\n",
    "            return docx.Document(data)\n",
    "        \n",
    "        elif tipo_archivo in ['excel', 'csv', 'parquet']:\n",
    "            # Leer con pandas según el formato\n",
    "            if tipo_archivo == 'excel':\n",
    "                return pd.read_excel(data)\n",
    "            elif tipo_archivo == 'csv':\n",
    "                return pd.read_csv(data)\n",
    "            elif tipo_archivo == 'parquet':\n",
    "                return pd.read_parquet(data)\n",
    "        \n",
    "    except S3Error as err:\n",
    "        print(f\"Error al acceder al archivo en MinIO: {err}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el archivo {ruta_archivo}: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        response.close()\n",
    "        response.release_conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e711e80e",
   "metadata": {},
   "source": [
    "conectarse base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import OperationalError\n",
    "\n",
    "def conectar_postgres(db_name, host, user, password, port=5432):\n",
    "    \"\"\"\n",
    "    Establece una conexión con una base de datos PostgreSQL.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    db_name : str\n",
    "        Nombre de la base de datos\n",
    "    host : str\n",
    "        Dirección del servidor (hostname o IP)\n",
    "    user : str\n",
    "        Nombre de usuario para la autenticación\n",
    "    password : str\n",
    "        Contraseña para la autenticación\n",
    "    port : int, opcional\n",
    "        Puerto de conexión (por defecto 5432)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    psycopg2.extensions.connection\n",
    "        Objeto de conexión a PostgreSQL\n",
    "    \n",
    "    Excepciones:\n",
    "    ------------\n",
    "    OperationalError\n",
    "        Si hay un error al conectar con la base de datos\n",
    "    ValueError\n",
    "        Si los parámetros requeridos no son proporcionados\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validar parámetros requeridos\n",
    "    if not all([db_name, host, user, password]):\n",
    "        raise ValueError(\"db_name, host, user y password son parámetros requeridos\")\n",
    "    \n",
    "    try:\n",
    "        # Establecer la conexión\n",
    "        conexion = psycopg2.connect(\n",
    "            dbname=db_name,\n",
    "            host=host,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            port=port,\n",
    "            client_encoding='UTF8'  # Forzar codificación UTF-8\n",
    "        )\n",
    "        \n",
    "        # Verificar la conexión\n",
    "        cursor = conexion.cursor()\n",
    "        cursor.execute(\"SELECT version();\")\n",
    "        version = cursor.fetchone()\n",
    "        cursor.close()\n",
    "        \n",
    "        print(f\"Conexión exitosa a PostgreSQL {version[0]}\")\n",
    "        return conexion\n",
    "    \n",
    "    except OperationalError as err:\n",
    "        print(f\"Error al conectar a PostgreSQL: {err}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f539d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def generar_ruta_fecha(separador='/', fecha=None):\n",
    "    \"\"\"\n",
    "    Genera un string con la fecha en formato año/mes/día.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    separador : str, opcional\n",
    "        Carácter separador entre componentes (por defecto '/')\n",
    "    fecha : datetime, opcional\n",
    "        Fecha específica a formatear (si None, usa fecha actual)\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    str\n",
    "        String con el formato 'YYYY{separador}MM{separador}DD'\n",
    "    \"\"\"\n",
    "    # Usar fecha actual si no se proporciona una específica\n",
    "    fecha_a_usar = fecha if fecha is not None else datetime.now()\n",
    "    \n",
    "    # Formatear la fecha\n",
    "    ruta_fecha = fecha_a_usar.strftime(f\"%Y{separador}%m{separador}%d\")\n",
    "    \n",
    "    return ruta_fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_columnas_destino(connection_params):\n",
    "    \"\"\"\n",
    "    Obtiene la lista de columnas de la tabla tab_ficharecolector2\n",
    "    \n",
    "    Parámetros:\n",
    "    - db_params: Diccionario con parámetros de conexión\n",
    "        \n",
    "    Retorna:\n",
    "    - Lista de nombres de columnas\n",
    "    \"\"\"\n",
    "    conexion = None\n",
    "    cursor = None\n",
    "    \n",
    "    try:\n",
    "        conexion = conectar_postgres(**connection_params)\n",
    "        cursor = conexion.cursor()\n",
    "        \n",
    "        # Consulta para obtener columnas\n",
    "        query = \"\"\"\n",
    "        SELECT column_name \n",
    "        FROM information_schema.columns \n",
    "        WHERE table_name = 'tab_ficharecolector2'\n",
    "        ORDER BY ordinal_position;\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        columnas = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        return columnas\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener columnas destino: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conexion:\n",
    "            conexion.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc303242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integracionpaciente(columnas_bd, connection_params):\n",
    "    \"\"\"\n",
    "    Integra datos de pacientes desde la tabla 'paciente' a 'tab_ficharecolector2'\n",
    "    \n",
    "    Parámetros:\n",
    "    - columnas_bd: Lista de columnas existentes en la tabla destino\n",
    "    - db_params: Diccionario con parámetros de conexión a la BD:\n",
    "        {\n",
    "            'db_name': 'nombre_bd',\n",
    "            'host': 'host_bd',\n",
    "            'user': 'usuario',\n",
    "            'password': 'contraseña',\n",
    "            'port': 5432 (opcional)\n",
    "        }\n",
    "    \n",
    "    Retorna:\n",
    "    - True si la integración fue exitosa\n",
    "    - False si hubo errores\n",
    "    \"\"\"\n",
    "    conexion = None\n",
    "    cursor = None\n",
    "    \n",
    "    try:\n",
    "        # Establecer conexión\n",
    "        conexion = conectar_postgres(**connection_params)\n",
    "        cursor = conexion.cursor()\n",
    "        \n",
    "        # 1. Obtener datos de pacientes\n",
    "        query_paciente = \"\"\"\n",
    "        SELECT \n",
    "            nombre1, nombre2, nombre3, \n",
    "            apellido1, apellido2, \n",
    "            sexo, id_paciente, fecha_nacimiento,\n",
    "            tipo_de_identificacion, identificacion,\n",
    "            municipio_residencia, barrio_residencia,\n",
    "            direccion, edad\n",
    "        FROM paciente;\n",
    "        \"\"\"\n",
    "        cursor.execute(query_paciente)\n",
    "        column_names = [desc[0] for desc in cursor.description]\n",
    "        pacientes = cursor.fetchall()\n",
    "        \n",
    "        # 2. Obtener IDs existentes para evitar duplicados\n",
    "        cursor.execute(\"SELECT llave_paciente FROM tab_ficharecolector2\")\n",
    "        ids_existentes = {row[0] for row in cursor.fetchall()}\n",
    "        \n",
    "        # 3. Preparar datos para inserción\n",
    "        mapeo_campos = {\n",
    "            'nombre1': 'prinom_paciente',\n",
    "            'nombre2': 'segunom_paciente',\n",
    "            'nombre3': 'tercernom_paciente',\n",
    "            'apellido1': 'primapellido_paciente',\n",
    "            'apellido2': 'seguapellido_paciente',\n",
    "            'sexo': 'sexo',\n",
    "            'id_paciente': 'llave_paciente',\n",
    "            'fecha_nacimiento': 'fenaci_paciente',\n",
    "            'tipo_de_identificacion': 'tipo_documento',\n",
    "            'identificacion': 'numid_paciente',\n",
    "            'municipio_residencia': 'municipio_resi1',\n",
    "            'barrio_residencia': 'comunabarrio_resi1',\n",
    "            'direccion': 'dir_residencia1',\n",
    "            'edad': 'edad_paciente'\n",
    "        }\n",
    "        \n",
    "        # 4. Filtrar y transformar datos\n",
    "        registros_a_insertar = []\n",
    "        for paciente in pacientes:\n",
    "            paciente_dict = dict(zip(column_names, paciente))\n",
    "            \n",
    "            # Verificar si el paciente ya existe\n",
    "            if paciente_dict['id_paciente'] in ids_existentes:\n",
    "                continue\n",
    "                \n",
    "            # Mapear campos al esquema destino\n",
    "            registro = {}\n",
    "            for origen, destino in mapeo_campos.items():\n",
    "                if destino in columnas_bd:  # Solo incluir columnas existentes\n",
    "                    registro[destino] = paciente_dict.get(origen)\n",
    "            \n",
    "            registros_a_insertar.append(registro)\n",
    "        \n",
    "        # 5. Insertar registros en lote\n",
    "        if registros_a_insertar:\n",
    "            columns = ', '.join(registros_a_insertar[0].keys())\n",
    "            placeholders = ', '.join(['%s'] * len(registros_a_insertar[0]))\n",
    "            \n",
    "            insert_query = f\"\"\"\n",
    "            INSERT INTO tab_ficharecolector2 ({columns})\n",
    "            VALUES ({placeholders})\n",
    "            \"\"\"\n",
    "            \n",
    "            # Preparar valores para inserción masiva\n",
    "            values = [tuple(registro.values()) for registro in registros_a_insertar]\n",
    "            \n",
    "            cursor.executemany(insert_query, values)\n",
    "            conexion.commit()\n",
    "            \n",
    "            print(f\"Se insertaron {len(registros_a_insertar)} nuevos registros de pacientes\")\n",
    "        else:\n",
    "            print(\"No hay nuevos registros de pacientes para insertar\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en integración de pacientes: {str(e)}\")\n",
    "        if conexion:\n",
    "            conexion.rollback()\n",
    "        return False\n",
    "        \n",
    "    finally:\n",
    "        # Cerrar cursor y conexión\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conexion:\n",
    "            conexion.close()\n",
    "            print(\"Conexión a PostgreSQL cerrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from psycopg2 import OperationalError\n",
    "\n",
    "def integracion_obs_infolab(connection_params):\n",
    "    \"\"\"\n",
    "    Integra observaciones desde la tabla infolab hacia tab_ficharecolector2.\n",
    "    Usa la función conectar_postgres con los parámetros proporcionados.\n",
    "    \"\"\"\n",
    "    connection = None\n",
    "\n",
    "    try:\n",
    "        # Usar directamente los params recibidos\n",
    "        connection = conectar_postgres(\n",
    "            db_name=connection_params['db_name'],\n",
    "            host=connection_params['host'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password'],\n",
    "            port=connection_params['port']\n",
    "        )\n",
    "\n",
    "        columnas_bd = obtener_columnas_destino(connection_params)\n",
    "        if not columnas_bd:\n",
    "            print(\"No se pudieron obtener las columnas destino\")\n",
    "            return False\n",
    "\n",
    "        # 1. Leer tabla infolab\n",
    "        query_infolab = \"\"\"\n",
    "        SELECT id_paciente, observacion, fecha_informe, fuente\n",
    "        FROM infolab_clean\n",
    "        ORDER BY id_paciente, fecha_informe;\n",
    "        \"\"\"\n",
    "        df_infolab = pd.read_sql(query_infolab, connection)\n",
    "\n",
    "        # 2. Leer tabla tab_ficharecolector2\n",
    "        query_ficha = \"\"\"\n",
    "        SELECT llave_paciente, obser_fue1, obser_fue2, obser_fue3, \n",
    "               obser_fue4, obser_fue5, obser_fue6, obser_fue7, obser_fue8\n",
    "        FROM tab_ficharecolector2;\n",
    "        \"\"\"\n",
    "        df_ficha = pd.read_sql(query_ficha, connection).drop_duplicates(subset='llave_paciente')\n",
    "\n",
    "        # 3. Agrupar y limitar por paciente\n",
    "        def limitar_y_rellenar(x): return list(x)[:8] + [np.nan] * (8 - len(x[:8]))\n",
    "\n",
    "        df_obs = df_infolab.groupby('id_paciente')['observacion'].apply(limitar_y_rellenar).apply(pd.Series)\n",
    "        df_fechas = df_infolab.groupby('id_paciente')['fecha_informe'].apply(limitar_y_rellenar).apply(pd.Series)\n",
    "        df_fuentes = df_infolab.groupby('id_paciente')['fuente'].apply(limitar_y_rellenar).apply(pd.Series)\n",
    "\n",
    "        df_obs.columns = [f\"obser_fue{i+1}\" for i in range(8)]\n",
    "        df_fechas.columns = [f\"fecha_fue{i+1}\" for i in range(8)]\n",
    "        df_fuentes.columns = [f\"fuente{i+1}\" for i in range(8)]\n",
    "\n",
    "        df_unificado = pd.concat([df_obs, df_fechas, df_fuentes], axis=1)\n",
    "        df_unificado.reset_index(inplace=True)\n",
    "\n",
    "        # 4. Merge con tabla ficharecolector\n",
    "        df_update = pd.merge(\n",
    "            df_ficha[['llave_paciente']],\n",
    "            df_unificado,\n",
    "            left_on='llave_paciente',\n",
    "            right_on='id_paciente',\n",
    "            how='left'\n",
    "        ).drop(columns=['id_paciente'])\n",
    "\n",
    "        # 5. Columnas destino\n",
    "        columnas_similares = [\n",
    "            col for col in [\n",
    "                \"obser_fue1\", \"obser_fue2\", \"obser_fue3\", \"obser_fue4\",\n",
    "                \"obser_fue5\", \"obser_fue6\", \"obser_fue7\", \"obser_fue8\",\n",
    "                \"fecha_fue1\", \"fecha_fue2\", \"fecha_fue3\", \"fecha_fue4\",\n",
    "                \"fecha_fue5\", \"fecha_fue6\", \"fecha_fue7\", \"fecha_fue8\",\n",
    "                \"fuente1\", \"fuente2\", \"fuente3\", \"fuente4\",\n",
    "                \"fuente5\", \"fuente6\", \"fuente7\", \"fuente8\"\n",
    "            ] if col in columnas_bd\n",
    "        ]\n",
    "\n",
    "        set_clause = ', '.join([f\"{col} = %({col})s\" for col in columnas_similares])\n",
    "        update_query = f\"\"\"\n",
    "        UPDATE tab_ficharecolector2\n",
    "        SET {set_clause}\n",
    "        WHERE llave_paciente = %(llave_paciente)s;\n",
    "        \"\"\"\n",
    "\n",
    "        # 6. Ejecutar UPDATE\n",
    "        with connection.cursor() as cur:\n",
    "            for _, row in df_update.iterrows():\n",
    "                update_data = {col: row[col] for col in columnas_similares + ['llave_paciente']}\n",
    "                cur.execute(update_query, update_data)\n",
    "            connection.commit()\n",
    "\n",
    "        print(\"Actualización completada con éxito.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "        print(f\"Error durante la integración: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()\n",
    "            print(\"Conexión cerrada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74cde9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from psycopg2 import OperationalError\n",
    "\n",
    "def integracion_obs_infolab_difuntos(connection_params):\n",
    "    \"\"\"\n",
    "    Integra observaciones desde la tabla infolab hacia tab_ficharecolector2.\n",
    "    Usa la función conectar_postgres con los parámetros proporcionados.\n",
    "    \"\"\"\n",
    "    connection = None\n",
    "\n",
    "    try:\n",
    "        # Usar directamente los params recibidos\n",
    "        connection = conectar_postgres(\n",
    "            db_name=connection_params['db_name'],\n",
    "            host=connection_params['host'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password'],\n",
    "            port=connection_params['port']\n",
    "        )\n",
    "\n",
    "        columnas_bd = obtener_columnas_destino(connection_params)\n",
    "        if not columnas_bd:\n",
    "            print(\"No se pudieron obtener las columnas destino\")\n",
    "            return False\n",
    "\n",
    "        # 1. Leer tabla infolab\n",
    "        query_infolab = \"\"\"\n",
    "        SELECT id_paciente, observaciones_fuente observacion, fecha_informe, 'DIF' fuente\n",
    "        FROM infolab_difuntos_clean\n",
    "        ORDER BY id_paciente;\n",
    "        \"\"\"\n",
    "        df_infolab = pd.read_sql(query_infolab, connection)\n",
    "\n",
    "        # 2. Leer tabla tab_ficharecolector2\n",
    "        query_ficha = \"\"\"\n",
    "        SELECT llave_paciente, obser_fue1, obser_fue2, obser_fue3, \n",
    "               obser_fue4, obser_fue5, obser_fue6, obser_fue7, obser_fue8\n",
    "        FROM tab_ficharecolector2;\n",
    "        \"\"\"\n",
    "        df_ficha = pd.read_sql(query_ficha, connection).drop_duplicates(subset='llave_paciente')\n",
    "\n",
    "        # 3. Agrupar y limitar por paciente\n",
    "        def limitar_y_rellenar(x): return list(x)[:8] + [np.nan] * (8 - len(x[:8]))\n",
    "\n",
    "        df_obs = df_infolab.groupby('id_paciente')['observacion'].apply(limitar_y_rellenar).apply(pd.Series)\n",
    "        df_fechas = df_infolab.groupby('id_paciente')['fecha_informe'].apply(limitar_y_rellenar).apply(pd.Series)\n",
    "        df_fuentes = df_infolab.groupby('id_paciente')['fuente'].apply(limitar_y_rellenar).apply(pd.Series)\n",
    "\n",
    "        df_obs.columns = [f\"obser_fue{i+1}\" for i in range(8)]\n",
    "        df_fechas.columns = [f\"fecha_fue{i+1}\" for i in range(8)]\n",
    "        df_fuentes.columns = [f\"fuente{i+1}\" for i in range(8)]\n",
    "\n",
    "        df_unificado = pd.concat([df_obs, df_fechas, df_fuentes], axis=1)\n",
    "        df_unificado.reset_index(inplace=True)\n",
    "\n",
    "        # 4. Merge con tabla ficharecolector\n",
    "        df_update = pd.merge(\n",
    "            df_ficha[['llave_paciente']],\n",
    "            df_unificado,\n",
    "            left_on='llave_paciente',\n",
    "            right_on='id_paciente',\n",
    "            how='left'\n",
    "        ).drop(columns=['id_paciente'])\n",
    "\n",
    "        # 5. Columnas destino\n",
    "        columnas_similares = [\n",
    "            col for col in [\n",
    "                \"obser_fue1\", \"obser_fue2\", \"obser_fue3\", \"obser_fue4\",\n",
    "                \"obser_fue5\", \"obser_fue6\", \"obser_fue7\", \"obser_fue8\",\n",
    "                \"fecha_fue1\", \"fecha_fue2\", \"fecha_fue3\", \"fecha_fue4\",\n",
    "                \"fecha_fue5\", \"fecha_fue6\", \"fecha_fue7\", \"fecha_fue8\",\n",
    "                \"fuente1\", \"fuente2\", \"fuente3\", \"fuente4\",\n",
    "                \"fuente5\", \"fuente6\", \"fuente7\", \"fuente8\"\n",
    "            ] if col in columnas_bd\n",
    "        ]\n",
    "\n",
    "        set_clause = ', '.join([f\"{col} = %({col})s\" for col in columnas_similares])\n",
    "        update_query = f\"\"\"\n",
    "        UPDATE tab_ficharecolector2\n",
    "        SET {set_clause}\n",
    "        WHERE llave_paciente = %(llave_paciente)s;\n",
    "        \"\"\"\n",
    "\n",
    "        # 6. Ejecutar UPDATE\n",
    "        with connection.cursor() as cur:\n",
    "            for _, row in df_update.iterrows():\n",
    "                update_data = {col: row[col] for col in columnas_similares + ['llave_paciente']}\n",
    "                cur.execute(update_query, update_data)\n",
    "            connection.commit()\n",
    "\n",
    "        print(\"Actualización completada con éxito.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "        print(f\"Error durante la integración: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()\n",
    "            print(\"Conexión cerrada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from psycopg2 import OperationalError\n",
    "\n",
    "def crear_df_ficharecolector(connection_params):\n",
    "    \"\"\"\n",
    "    Integra observaciones desde la tabla infolab hacia tab_ficharecolector2.\n",
    "    Usa la función conectar_postgres con los parámetros proporcionados.\n",
    "    \"\"\"\n",
    "    connection = None\n",
    "\n",
    "    try:\n",
    "        # Usar directamente los params recibidos\n",
    "        connection = conectar_postgres(\n",
    "            db_name=connection_params['db_name'],\n",
    "            host=connection_params['host'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password'],\n",
    "            port=connection_params['port']\n",
    "        )\n",
    "\n",
    "        # 1. Leer tabla infolab\n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM tab_ficharecolector2\n",
    "        WHERE llave_paciente IS NOT NULL;\n",
    "        \"\"\"\n",
    "\n",
    "        df= pd.read_sql(query, connection)\n",
    "        if df.empty:\n",
    "            print(\"No se encontraron registros en tab_ficharecolector2.\")\n",
    "            return None\n",
    "        return df\n",
    "    except OperationalError as e:\n",
    "        print(f\"Error al conectar a la base de datos: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#print(pd.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
